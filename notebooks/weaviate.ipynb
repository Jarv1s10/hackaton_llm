{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "import weaviate\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationTokenBufferMemory, ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "WEAVIATE_API_KEY = os.getenv('WEAVIATE_API_KEY')\n",
    "WEAVIATE_URL = os.getenv('WEAVIATE_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(\n",
    "    url=WEAVIATE_URL,\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "db = Weaviate(client, 'RGDocs', 'text', embeddings, by_text=False)\n",
    "db._query_attrs = [p['name'] for p in client.schema.get('RGDocs')['properties']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What do you know about Revenue Grid?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query[query.find('do ')+len('do '):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query, by_text=False)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)\n",
    "docs[0].metadata['doc_title'], docs[0].metadata['doc_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.llms import BaseLLM\n",
    "\n",
    "class DummyLLM(BaseLLM):\n",
    "    _tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "    def predict(self, text):\n",
    "        return 'This is LLM Response'\n",
    "\n",
    "    def get_num_tokens(self, text):\n",
    "        return len(self._tokenizer.encode(text))\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts,\n",
    "        stop = None,\n",
    "        run_manager = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Run the LLM on the given prompts.\"\"\"\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        prompts,\n",
    "        stop = None,\n",
    "        run_manager = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Run the LLM on the given prompts.\"\"\"\n",
    "\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Return type of llm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DummyLLM()\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \\\n",
    "\"\"\"You are an AI assistant for the Revenue Grid documentation.\n",
    "You are given a question and extracted parts of product documentation. Provide a conversational answer to the question using the pieces of information provided.\n",
    "If the question includes a request for code, provide a code block directly from the documentation.\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "If the question is not about Revenue Grid, politely inform them that you are tuned to only answer questions about Revenue Grid.\n",
    "\"\"\"\n",
    "TOKEN_LIMIT = 1000\n",
    "def process_query(query):\n",
    "    global memory\n",
    "\n",
    "    # memory.append(f'Question: {query}')\n",
    "    history = memory.load_memory_variables({})['history'] + '\\n' + f'Human: {query}'\n",
    "    print(f'{history = }')\n",
    "    # relevant_docs = db.similarity_search(history, by_text=False)\n",
    "    relevant_docs = docs\n",
    "\n",
    "    while sum([llm.get_num_tokens(doc.page_content) for doc in relevant_docs]) > TOKEN_LIMIT:\n",
    "        relevant_docs = relevant_docs[:-1]\n",
    "\n",
    "    summaries = [doc.page_content for doc in relevant_docs]\n",
    "    summaries = '\\n'.join(summaries)\n",
    "\n",
    "    sources = 'Sources:\\n' + '\\n'.join([f\"[{doc.metadata['doc_title']}]({doc.metadata['doc_url']})\" for doc in relevant_docs])\n",
    "\n",
    "    prompt_to_llm = SYSTEM_MESSAGE + history + 'Summaries:\\n' + summaries\n",
    "    print(f'{prompt_to_llm = }')\n",
    "    llm_answer = llm.predict(prompt_to_llm)\n",
    "    display_answer = llm_answer + '\\n--------------\\n' + sources\n",
    "\n",
    "    # memory.append(f'Answer: {llm_answer}')\n",
    "    memory.save_context({'input': query}, {'output': llm_answer})\n",
    "\n",
    "    return display_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{process_query(query) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})['history'], llm.get_num_tokens(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
