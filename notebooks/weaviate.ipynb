{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import weaviate\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.llms import OpenAI, FakeListLLM\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationTokenBufferMemory, ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('../app/api/.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "WEAVIATE_API_KEY = os.getenv('WEAVIATE_API_KEY')\n",
    "WEAVIATE_URL = os.getenv('WEAVIATE_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(\n",
    "    url=WEAVIATE_URL,\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "db = Weaviate(client, 'RGDocs', 'text', embeddings, by_text=False, attributes=['doc_title', 'doc_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.query.get('RGDocs', ['text', 'doc_title', 'doc_url']).with_where({\n",
    "    'path': ['doc_url'],\n",
    "    'operator': 'Equal',\n",
    "    'valueString': \"https://revenuegrid.com/ssi/docs/kb/articles/Privacy-and-Securityqwe\"\n",
    "}).do()['data']['Get']['RGDocs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batch.delete_objects(\n",
    "    'RGDocs',\n",
    "    {\n",
    "    'path': ['doc_url'],\n",
    "    'operator': 'Equal',\n",
    "    'valueString': \"https://revenuegrid.com/ssi/docs/kb/articles/Privacy-and-Security\"\n",
    "    },\n",
    "    output='verbose',\n",
    "    dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://0.0.0.0:8000/pages/'\n",
    "params = {\n",
    "    'doc_url': 'https://revenuegrid.com/ssi/docs/kb/articles/Privacy-and-Security'\n",
    "}\n",
    "headers = {\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "response = requests.post(url, params=params, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'doc_url': 'https://revenuegrid.com/ssi/docs/kb/articles/Privacy-and-Security',\n",
    "    'dry_run': True,\n",
    "}\n",
    "r = requests.delete('http://127.0.0.1:8000/pages/delete', params=params, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_answer': 'fake llm response',\n",
       " 'sources': [{'doc_title': 'Salesforce integration Knowledge Base',\n",
       "   'doc_url': 'https://docs.revenuegrid.com/'},\n",
       "  {'doc_title': 'Release notes',\n",
       "   'doc_url': 'https://docs.revenuegrid.com/articles/release-notes-intelligence'},\n",
       "  {'doc_title': 'How to open Revenue Grid',\n",
       "   'doc_url': 'https://docs.revenuegrid.com/articles/Setup'},\n",
       "  {'doc_title': 'Integration with Salesforce Customer / Partner Community',\n",
       "   'doc_url': 'https://docs.revenuegrid.com/ri/fast/articles/Partner-Community-Integration'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What do you know about Revenue Grid?'\n",
    "\n",
    "data = {\n",
    "    'user_message': query,\n",
    "    'history': [('qwe', 'rty')]\n",
    "}\n",
    "headers = {\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "r = requests.post('http://localhost:8000/chat/', json=data, headers=headers)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query[query.find('do ')+len('do '):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query, by_text=False)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)\n",
    "docs[0].metadata['doc_title'], docs[0].metadata['doc_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain.llms import BaseLLM\n",
    "\n",
    "class DummyLLM(BaseLLM):\n",
    "    _tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "\n",
    "    def predict(self, text):\n",
    "        return 'This is LLM Response'\n",
    "\n",
    "    def get_num_tokens(self, text):\n",
    "        return len(self._tokenizer.encode(text))\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts,\n",
    "        stop = None,\n",
    "        run_manager = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Run the LLM on the given prompts.\"\"\"\n",
    "\n",
    "    async def _agenerate(\n",
    "        self,\n",
    "        prompts,\n",
    "        stop = None,\n",
    "        run_manager = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Run the LLM on the given prompts.\"\"\"\n",
    "\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Return type of llm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DummyLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({'input': 'query'}, {'output': 'llm_answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='query', additional_kwargs={}, example=False), AIMessage(content='llm_answer', additional_kwargs={}, example=False)])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "ChatMessageHistory.from_obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_memory': {'messages': [{'content': 'query',\n",
       "    'additional_kwargs': {},\n",
       "    'example': False},\n",
       "   {'content': 'llm_answer', 'additional_kwargs': {}, 'example': False}]},\n",
       " 'output_key': None,\n",
       " 'input_key': None,\n",
       " 'return_messages': False,\n",
       " 'human_prefix': 'Human',\n",
       " 'ai_prefix': 'AI',\n",
       " 'memory_key': 'history',\n",
       " 'max_token_limit': 50}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "d = deepcopy(memory.dict())\n",
    "# d['llm'] = llm\n",
    "del d['llm']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for ConversationTokenBufferMemory\nchat_memory\n  instance of BaseChatMessageHistory expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseChatMessageHistory)\nllm\n  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, predict, predict_messages (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ConversationTokenBufferMemory\u001b[39m.\u001b[39;49mparse_obj(memory\u001b[39m.\u001b[39;49mdict())\n",
      "File \u001b[0;32m~/projects/personal/hackaton_06_23/task2/.venv/lib/python3.10/site-packages/pydantic/main.py:526\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/projects/personal/hackaton_06_23/task2/.venv/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/projects/personal/hackaton_06_23/task2/.venv/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for ConversationTokenBufferMemory\nchat_memory\n  instance of BaseChatMessageHistory expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseChatMessageHistory)\nllm\n  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, predict, predict_messages (type=type_error)"
     ]
    }
   ],
   "source": [
    "ConversationTokenBufferMemory.parse_obj(memory.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \\\n",
    "\"\"\"You are an AI assistant for the Revenue Grid documentation.\n",
    "You are given a question and extracted parts of product documentation. Provide a conversational answer to the question using the pieces of information provided.\n",
    "If the question includes a request for code, provide a code block directly from the documentation.\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "If the question is not about Revenue Grid, politely inform them that you are tuned to only answer questions about Revenue Grid.\n",
    "\"\"\"\n",
    "TOKEN_LIMIT = 1000\n",
    "def process_query(query):\n",
    "    global memory\n",
    "\n",
    "    # memory.append(f'Question: {query}')\n",
    "    history = memory.load_memory_variables({})['history'] + '\\n' + f'Human: {query}'\n",
    "    print(f'{history = }')\n",
    "    # relevant_docs = db.similarity_search(history, by_text=False)\n",
    "    relevant_docs = docs\n",
    "\n",
    "    while sum([llm.get_num_tokens(doc.page_content) for doc in relevant_docs]) > TOKEN_LIMIT:\n",
    "        relevant_docs = relevant_docs[:-1]\n",
    "\n",
    "    summaries = [doc.page_content for doc in relevant_docs]\n",
    "    summaries = '\\n'.join(summaries)\n",
    "\n",
    "    sources = 'Sources:\\n' + '\\n'.join([f\"[{doc.metadata['doc_title']}]({doc.metadata['doc_url']})\" for doc in relevant_docs])\n",
    "\n",
    "    prompt_to_llm = SYSTEM_MESSAGE + history + 'Summaries:\\n' + summaries\n",
    "    print(f'{prompt_to_llm = }')\n",
    "    llm_answer = llm.predict(prompt_to_llm)\n",
    "    display_answer = llm_answer + '\\n--------------\\n' + sources\n",
    "\n",
    "    # memory.append(f'Answer: {llm_answer}')\n",
    "    memory.save_context({'input': query}, {'output': llm_answer})\n",
    "\n",
    "    return display_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{process_query(query) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})['history'], llm.get_num_tokens(memory.load_memory_variables({})['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.chat_memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
